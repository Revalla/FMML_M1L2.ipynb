{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Revalla/FMML_M1L2.ipynb/blob/main/FMML_M1L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine Learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2\n",
        "\n",
        "In this lab, we will show a part of the ML pipeline by using the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district. We will use the scikit-learn library to load the data and perform some basic data preprocessing and model training. We will also show how to evaluate the model using some common metrics, split the data into training and testing sets, and use cross-validation to get a better estimate of the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LpqjN991GGJ",
        "outputId": "d9d73fe5-9d26-4992-e10e-263d665ba599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _california_housing_dataset:\n",
            "\n",
            "California Housing dataset\n",
            "--------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 20640\n",
            "\n",
            "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
            "\n",
            "    :Attribute Information:\n",
            "        - MedInc        median income in block group\n",
            "        - HouseAge      median house age in block group\n",
            "        - AveRooms      average number of rooms per household\n",
            "        - AveBedrms     average number of bedrooms per household\n",
            "        - Population    block group population\n",
            "        - AveOccup      average number of household members\n",
            "        - Latitude      block group latitude\n",
            "        - Longitude     block group longitude\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "This dataset was obtained from the StatLib repository.\n",
            "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
            "\n",
            "The target variable is the median house value for California districts,\n",
            "expressed in hundreds of thousands of dollars ($100,000).\n",
            "\n",
            "This dataset was derived from the 1990 U.S. census, using one row per census\n",
            "block group. A block group is the smallest geographical unit for which the U.S.\n",
            "Census Bureau publishes sample data (a block group typically has a population\n",
            "of 600 to 3,000 people).\n",
            "\n",
            "A household is a group of people residing within a home. Since the average\n",
            "number of rooms and bedrooms in this dataset are provided per household, these\n",
            "columns may take surprisingly large values for block groups with few households\n",
            "and many empty houses, such as vacation resorts.\n",
            "\n",
            "It can be downloaded/loaded using the\n",
            ":func:`sklearn.datasets.fetch_california_housing` function.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
            "      Statistics and Probability Letters, 33 (1997) 291-297\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = datasets.fetch_california_housing()\n",
        "# Dataset description\n",
        "print(dataset.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCe1VNftevgE"
      },
      "source": [
        "Given below are the list of target values. These correspond to the house value derived considering all the 8 input features and are continuous values. We should use regression models to predict these values but we will start with a simple classification model for the sake of simplicity. We need to just round off the values to the nearest integer and use a classification model to predict the house value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8K0ggBOevgE",
        "outputId": "e4fe6838-c9ed-49cb-e9ca-aedc75d79b82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orignal target values: [4.526 3.585 3.521 ... 0.923 0.847 0.894]\n",
            "Target values after conversion: [4 3 3 ... 0 0 0]\n",
            "Input variables shape: (20640, 8)\n",
            "Output variables shape: (20640,)\n"
          ]
        }
      ],
      "source": [
        "print(\"Orignal target values:\", dataset.target)\n",
        "\n",
        "dataset.target = dataset.target.astype(int)\n",
        "\n",
        "print(\"Target values after conversion:\", dataset.target)\n",
        "print(\"Input variables shape:\", dataset.data.shape)\n",
        "print(\"Output variables shape:\", dataset.target.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "The simplest model to use for classification is the K-Nearest Neighbors model. We will use this model to predict the house value with a K value of 1. We will also use the accuracy metric to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "outputs": [],
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and a query point\n",
        "    and returns the predicted label for the query point using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    query: numpy array of shape (d,) where d is the number of features\n",
        "\n",
        "    returns: the predicted label for the query point which is the label of the training data which is closest to the query point\n",
        "    \"\"\"\n",
        "    diff = (\n",
        "        traindata - query\n",
        "    )  # find the difference between features. Numpy automatically takes care of the size here\n",
        "    sq = diff * diff  # square the differences\n",
        "    dist = sq.sum(1)  # add up the squares\n",
        "    label = trainlabel[np.argmin(dist)]\n",
        "    return label\n",
        "\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is the label of the training data which is closest to each test point\n",
        "    \"\"\"\n",
        "    predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "    return predlabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "outputs": [],
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the random classifier algorithm\n",
        "\n",
        "    In reality, we don't need these arguments but we are passing them to keep the function signature consistent with other classifiers\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is a random label from the training data\n",
        "    \"\"\"\n",
        "\n",
        "    classes = np.unique(trainlabel)\n",
        "    rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "    predlabel = classes[rints]\n",
        "    return predlabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "We need a metric to evaluate the performance of the model. Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm. We will use the accuracy metric to evaluate and compate the performance of the K-Nearest Neighbors model and the random classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "outputs": [],
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "    \"\"\"\n",
        "    This function takes in the ground-truth labels and predicted labels\n",
        "    and returns the accuracy of the classifier\n",
        "\n",
        "    gtlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    predlabel: numpy array of shape (n,) where n is the number of samples\n",
        "\n",
        "    returns: the accuracy of the classifier which is the number of correct predictions divided by the total number of predictions\n",
        "    \"\"\"\n",
        "    assert len(gtlabel) == len(\n",
        "        predlabel\n",
        "    ), \"Length of the ground-truth labels and predicted labels should be the same\"\n",
        "    correct = (\n",
        "        gtlabel == predlabel\n",
        "    ).sum()  # count the number of times the groundtruth label is equal to the predicted label.\n",
        "    return correct / len(gtlabel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability. We will use this function to split the dataset into training and testing sets. We will use the training set to train the model and the testing set to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "outputs": [],
      "source": [
        "def split(data, label, percent):\n",
        "    # generate a random number for each sample\n",
        "    rnd = rng.random(len(label))\n",
        "    split1 = rnd < percent\n",
        "    split2 = rnd >= percent\n",
        "\n",
        "    split1data = data[split1, :]\n",
        "    split1label = label[split1]\n",
        "    split2data = data[split2, :]\n",
        "    split2label = label[split2]\n",
        "    return split1data, split1label, split2data, split2label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBZkHBLJ1iU-",
        "outputId": "f40352c3-71e7-4075-b922-577bc5c3493c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples: 4144\n",
            "Number of train samples: 16496\n",
            "Percent of test data: 20.07751937984496 %\n"
          ]
        }
      ],
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(\n",
        "    dataset.data, dataset.target, 20 / 100\n",
        ")\n",
        "print(\"Number of test samples:\", len(testlabel))\n",
        "print(\"Number of train samples:\", len(alltrainlabel))\n",
        "print(\"Percent of test data:\", len(testlabel) * 100 / len(dataset.target), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "outputs": [],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(\n",
        "    alltraindata, alltrainlabel, 75 / 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBlZDTHUFTZx",
        "outputId": "03ca6e5b-c54d-4a30-b35a-bda0d155b830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy using nearest neighbour algorithm: 100.0 %\n",
            "Training accuracy using random classifier:  16.4375808538163 %\n"
          ]
        }
      ],
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using nearest neighbour algorithm:\", trainAccuracy*100, \"%\")\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using random classifier: \", trainAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case. This is because the random classifier randomly assigns a label to each sample and the probability of assigning the correct label is 1/(number of classes). Let us predict the labels for our validation set and get the accuracy. This accuracy is a good estimate of the accuracy of our model on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h7bXoW_2H3v",
        "outputId": "5e63b2de-04f3-4cbf-e9f7-56bcc247b8c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour algorithm: 34.10852713178294 %\n",
            "Validation accuracy using random classifier: 16.884689922480618 %\n"
          ]
        }
      ],
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour algorithm:\", valAccuracy*100, \"%\")\n",
        "\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier:\", valAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier. Now let us try another random split and check the validation accuracy. We will see that the validation accuracy changes with the split. This is because the validation set is small and the accuracy is highly dependent on the samples in the validation set. We can get a better estimate of the accuracy by using cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujm3cyYzEntE",
        "outputId": "2ada2219-7159-4e9e-bb68-7e3a193baf5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour algorithm: 34.048257372654156 %\n"
          ]
        }
      ],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(\n",
        "    alltraindata, alltrainlabel, 75 / 100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour algorithm:\", valAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEZ5ToYBEDW",
        "outputId": "1968e805-407e-4070-d68b-8588cd32b871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 34.91795366795367 %\n"
          ]
        }
      ],
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "\n",
        "print(\"Test accuracy:\", testAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9zvdYY6evgI"
      },
      "source": [
        "> Exercise: Try to implement a 3 nearest neighbour classifier and compare the accuracy of the 1 nearest neighbour classifier and the 3 nearest neighbour classifier on the test dataset. You can use the KNeighborsClassifier class from the scikit-learn library to implement the K-Nearest Neighbors model. You can set the number of neighbors using the n_neighbors parameter. You can also use the accuracy_score function from the scikit-learn library to calculate the accuracy of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>cross-validation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute. You can reduce the number of splits to make it faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "outputs": [],
      "source": [
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "    \"\"\"\n",
        "    This function takes in the data, labels, split percentage, number of iterations and classifier function\n",
        "    and returns the average accuracy of the classifier\n",
        "\n",
        "    alldata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    alllabel: numpy array of shape (n,) where n is the number of samples\n",
        "    splitpercent: float which is the percentage of data to be used for training\n",
        "    iterations: int which is the number of iterations to run the classifier\n",
        "    classifier: function which is the classifier function to be used\n",
        "\n",
        "    returns: the average accuracy of the classifier\n",
        "    \"\"\"\n",
        "    accuracy = 0\n",
        "    for ii in range(iterations):\n",
        "        traindata, trainlabel, valdata, vallabel = split(\n",
        "            alldata, alllabel, splitpercent\n",
        "        )\n",
        "        valpred = classifier(traindata, trainlabel, valdata)\n",
        "        accuracy += Accuracy(vallabel, valpred)\n",
        "    return accuracy / iterations  # average of all accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3qtNar7Bbik",
        "outputId": "339d7655-d08f-4a15-f295-e17ae03daa60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy: 33.58463539517022 %\n",
            "Test accuracy: 34.91795366795367 %\n"
          ]
        }
      ],
      "source": [
        "avg_acc = AverageAccuracy(alltraindata, alltrainlabel, 75 / 100, 10, classifier=NN)\n",
        "print(\"Average validation accuracy:\", avg_acc*100, \"%\")\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "\n",
        "print(\"Test accuracy:\", Accuracy(testlabel, testpred)*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "Yes, averaging the validation accuracy across multiple splits generally gives more consistent and reliable results in machine learning, especially in the context of cross-validation.\n",
        "\n",
        "### Why Averaging Over Multiple Splits is Beneficial\n",
        "\n",
        "1. **Reduces Variance**: When you split your dataset into training and validation sets, the performance of your model can vary significantly depending on the specific split. Certain splits might be easier or harder for the model, leading to high variance in validation accuracy. By averaging the accuracy over multiple splits (like in k-fold cross-validation), you reduce the impact of any one split being particularly easy or hard. This gives a more stable estimate of model performance.\n",
        "\n",
        "2. **More Representative Performance**: Averaging over multiple splits helps ensure that the performance metric reflects how the model will generalize to an independent dataset, as each fold is used as both training and validation at different times. This makes the averaged accuracy more representative of the model's expected performance on unseen data.\n",
        "\n",
        "3. **Balances Data Distribution**: Different splits can capture different aspects of data variability (e.g., different classes, feature distributions). Averaging across multiple splits helps to ensure that the performance estimate takes into account the overall distribution of the data rather than being overly influenced by a single partition.\n",
        "\n",
        "### Common Approaches to Averaging Validation Accuracy\n",
        "\n",
        "- **k-Fold Cross-Validation**: The dataset is divided into `k` equally sized folds. The model is trained on `k-1` folds and validated on the remaining fold. This process is repeated `k` times, with each fold being used as the validation set once. The validation accuracy is averaged over the `k` iterations to provide a single performance metric.\n",
        "\n",
        "- **Repeated k-Fold Cross-Validation**: This involves repeating the k-fold cross-validation process multiple times with different random splits. The final accuracy is the average of all iterations. This method can provide even more robust estimates, especially for small datasets.\n",
        "\n",
        "- **Stratified k-Fold Cross-Validation**: Similar to k-fold, but ensures that each fold maintains the same proportion of each class as the original dataset. This is particularly useful for imbalanced datasets to ensure that each fold is representative.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Averaging validation accuracy across multiple splits, such as in cross-validation, generally leads to more reliable and consistent estimates of a model's performance. It mitigates the effects of variability due to different data splits and provides a better understanding of how the model will generalize to unseen data.\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "Yes, averaging validation accuracy across multiple splits typically gives a more accurate estimate of test accuracy. This approach, commonly used in cross-validation, is considered more reliable because it better represents how the model will perform on unseen data. Here's why:\n",
        "\n",
        "### 1. **Reduces Overfitting to Specific Data Splits**\n",
        "When a model is trained and validated on a single split of the data, the resulting accuracy can be overly influenced by the specific examples included in that split. If the validation set happens to be particularly easy or difficult, the accuracy might be higher or lower than what you would see on the test set. By averaging the accuracy over multiple splits, such as in k-fold cross-validation, you reduce the effect of any single, potentially unrepresentative, split. This gives a more balanced estimate of the model's true performance.\n",
        "\n",
        "### 2. **Covers More Data Variability**\n",
        "Each fold in cross-validation serves as a different validation set, which helps the model be evaluated on multiple subsets of data. This allows the accuracy estimate to account for different aspects of the data distribution, including any variability or noise. It ensures that the performance metrics are not dependent on any particular subset of the data, leading to a more robust and generalizable estimate of the test accuracy.\n",
        "\n",
        "### 3. **Utilizes the Entire Dataset**\n",
        "In k-fold cross-validation, every data point is used for both training and validation at different stages. This comprehensive use of the dataset ensures that the model is exposed to as much information as possible, providing a more thorough evaluation of its performance across all possible data points. This holistic view typically results in an accuracy estimate that is closer to what would be seen on an independent test set.\n",
        "\n",
        "### 4. **Mitigates the Impact of Randomness**\n",
        "When using a single train-test split, the random selection of data points can lead to a biased performance estimate. For example, the validation set might accidentally include more outliers or more examples of certain classes. By using multiple splits and averaging the results, cross-validation reduces the impact of this randomness, producing a more accurate estimate of how the model will perform on genuinely new data.\n",
        "\n",
        "### Conclusion\n",
        "Averaging validation accuracy across multiple splits, as done in cross-validation, generally provides a more accurate and reliable estimate of test accuracy. It reduces the influence of random variations in the data splits, ensures the model is evaluated on a broader range of data, and offers a better assessment of the model's ability to generalize to unseen data.\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "Yes, generally speaking, increasing the number of iterations in cross-validation or repeated validation methods can lead to a better estimate of the model's performance on unseen data. However, the relationship between the number of iterations and the accuracy of the estimate has diminishing returns beyond a certain point. Let's explore this in more detail.\n",
        "\n",
        "### Effects of Increasing the Number of Iterations on the Estimate\n",
        "\n",
        "1. **Reduction in Variance**:\n",
        "   - **Fewer Iterations**: When you perform a small number of iterations (e.g., low `k` in k-fold cross-validation), the variance of your estimate is higher. This means the estimated performance (like accuracy) might fluctuate more between different runs due to the random sampling of training and validation sets.\n",
        "   - **More Iterations**: Increasing the number of iterations (e.g., using more folds in k-fold cross-validation or repeating cross-validation multiple times) tends to reduce the variance of the estimate. This leads to a more stable and reliable estimate of the model's performance because the model is evaluated on more diverse subsets of data, covering a wider range of possible data distributions.\n",
        "\n",
        "2. **Improved Generalization Estimate**:\n",
        "   - More iterations provide a more comprehensive assessment of the model's ability to generalize to unseen data. By evaluating the model on multiple different subsets of the data, we can obtain a better sense of its true generalization performance, especially in the presence of data variability or imbalances.\n",
        "\n",
        "3. **Law of Diminishing Returns**:\n",
        "   - While increasing the number of iterations generally improves the estimate by reducing variance, there is a point of diminishing returns. After a certain number of iterations, the gain in the accuracy of the estimate becomes minimal. Beyond this point, further increasing the number of iterations does not significantly enhance the estimate but will increase computational cost.\n",
        "\n",
        "4. **Computational Cost**:\n",
        "   - **Fewer Iterations**: Lower computational cost but higher variance in the estimate.\n",
        "   - **More Iterations**: Higher computational cost but lower variance and a potentially more accurate estimate. The additional computational cost might not be justified if the performance gain is negligible.\n",
        "\n",
        "### Practical Considerations\n",
        "\n",
        "- **k-Fold Cross-Validation**: Common values for `k` are 5 or 10. Increasing `k` beyond 10 usually provides only marginal improvements in the performance estimate but significantly increases computational time.\n",
        "  \n",
        "- **Repeated k-Fold Cross-Validation**: Repeating k-fold cross-validation multiple times with different random splits (e.g., repeating 10-fold cross-validation 5 times) can provide more reliable performance estimates, especially in cases where the dataset is small or noisy.\n",
        "\n",
        "- **Leave-One-Out Cross-Validation (LOOCV)**: This is an extreme form where `k` equals the number of data points. While LOOCV provides a very low-bias estimate, it is computationally expensive and can have high variance in the estimate because each fold is based on a single data point.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Increasing the number of iterations in cross-validation generally leads to a more accurate estimate of the model's performance by reducing variance and providing a better sense of generalization ability. However, the benefit diminishes beyond a certain point, and the computational cost increases. In practice, a balance is often struck by using a reasonable number of folds (like 5 or 10) or repeating cross-validation a few times to achieve a reliable estimate without excessive computation.\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n",
        "Yes, increasing the number of iterations in cross-validation can help deal with very small training or validation datasets, but there are important nuances to consider. Let's break down how increasing iterations can help and what limitations still exist:\n",
        "\n",
        "### How Increasing Iterations Helps with Small Datasets\n",
        "\n",
        "1. **Maximizes Data Usage**:\n",
        "   - In scenarios with small datasets, the amount of data available for training and validation is limited. Increasing the number of iterations in cross-validation ensures that each data point is used in both the training and validation sets across different iterations. This helps make the most of a limited dataset by effectively utilizing all available data.\n",
        "\n",
        "2. **Reduces Variance in Performance Estimates**:\n",
        "   - With small datasets, the results of a single train-test split can vary significantly due to the small number of samples in each set. By increasing the number of iterations, such as using k-fold cross-validation with a higher `k` (e.g., `k = n`, where `n` is the number of data points in Leave-One-Out Cross-Validation), you reduce the variance in the performance estimates. Each fold will have more opportunities to serve as both training and validation sets, leading to a more robust and stable performance estimate.\n",
        "\n",
        "3. **Provides More Robust Performance Estimates**:\n",
        "   - Repeated cross-validation or k-fold cross-validation (with an appropriate value for `k`) can provide a more accurate measure of a model's performance, especially when data is scarce. This approach gives a more comprehensive view of how the model performs across different subsets of the data, which can be particularly valuable when the dataset is small and each sample's influence is substantial.\n",
        "\n",
        "### Limitations of Increasing Iterations with Small Datasets\n",
        "\n",
        "1. **Small Validation Sets Lead to High Variance Estimates**:\n",
        "   - If the training dataset is small, each validation fold in cross-validation will also be small. Small validation sets can lead to high variance in performance estimates since the model's accuracy on a small set can fluctuate significantly depending on which samples are included. Increasing the number of iterations can somewhat mitigate this effect by averaging the results, but the fundamental issue of high variance due to small validation sets remains.\n",
        "\n",
        "2. **Risk of Overfitting**:\n",
        "   - With very small training datasets, there is a higher risk of overfitting. The model may learn patterns that are specific to the limited training data and do not generalize well to new data. Increasing the number of iterations does not address this problem directly; rather, it only provides a more stable estimate of the overfitted performance. Regularization techniques and simpler models are often needed to combat overfitting with small datasets.\n",
        "\n",
        "3. **Computational Cost**:\n",
        "   - For very small datasets, techniques like Leave-One-Out Cross-Validation (LOOCV) (where `k` equals the number of data points) can be computationally expensive, especially for models with high training complexity. Although each iteration trains on nearly all data points except one, the large number of iterations (one per data point) can lead to high computational costs.\n",
        "\n",
        "4. **Limited Information**:\n",
        "   - If the dataset is very small, there might simply not be enough information to learn a robust model, regardless of the number of iterations. Cross-validation can help provide a better estimate of performance, but it cannot create new information where there is none.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Increasing the number of iterations in cross-validation (e.g., using k-fold cross-validation or repeated cross-validation) can help make the most of small datasets by providing more reliable performance estimates and reducing variance. However, this approach cannot overcome the fundamental limitations of small datasets, such as high variance in validation results, risk of overfitting, and limited information. To effectively work with small datasets, it is often necessary to combine cross-validation with other strategies, such as regularization, data augmentation, simpler models, or even domain-specific knowledge to enhance the learning process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-SBxy1qevgJ"
      },
      "source": [
        "> Exercise: How does the accuracy of the 3 nearest neighbour classifier change with the number of splits? How is it affected by the split size? Compare the results with the 1 nearest neighbour classifier."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}